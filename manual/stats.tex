\normalsize
\subsection[stats]{stats.py}
Simple statistical analysis of mono-dimensional sets of data, arranged in different tools: statistics (\func{stats},
\func{autocorrelation}, \func{sampling\_ratio}), clustering (\func{k\_means}), probability (\func{student},
\func{fisher}, \func{normal}, \func{inverse\_*}, \func{*\_test}) and principal component analysis (\func{PCA}).

\begin{pyglist}[language=python,fvset={frame=single}]
def stats( x )

def autocorrelation( x, k = 1 )

# The value of the sampling ratio that arises from any given data sequence is the factor 
# by which the number of configurations sampled must be increased in order to obtain the
# same precision that would result from randomly distributed data points.
def sampling_ratio( x )

class k_means( x )
    def find_centers( k )

def student( x, v )

def fisher( x, v1, v2 )

def normal( x, m = 0.0, s = 1.0 )

def inverse_student( alfa, v )

def inverse_fisher( alfa, v1, v2 )

def inverse_normal( alfa, m = 0.0, s = 1.0 )

# Check for both samples having the same mean
#        (unpaired data without common values between subsets)
def t_test( na, ma, sa, nb, mb, sb, a = 0.05 )

# Check for both samples having the same variance
#        (unpaired data without common values between subsets)
def f_test( na, sa, nb, sb, a = 0.05 )

# Definition based on t-Student, so kinda compatible results...
def i_test( na, ma, sa, nb, mb, sb, a = 0.05 )

class PCA( x )
    def select( sel )
\end{pyglist}

\footnotesize
\begin{pyglist}[language=python,fvset={frame=single}]
>>> import qm3.maths.stats as stats
>>> import qm3.maths.matrix as matrix

>>> stats.stats( [ 0.388, 0.389, 0.410 ] )
(0.39566666666666667, 0.01242309676905613)

>>> stats.autocorrelation( range( 10 ) )
0.7

>>> stats.autocorrelation( range( 10 ), 2 )
0.4121212121212121

>>> stats.sampling_ratio( range( 10 ) )
5.666666666666666

>>> stats.normal( 1.80 )
0.07895015830089415

>>> import integration
>>> integration.Simpson_f( lambda x: stats.normal( x, 0.0, 1.0 ), -100.0, 0.09 )[0]
0.5358563925911398

>>> stats.student( 1.80, 30 )
0.08070962479849014

>>> stats.inverse_normal( 0.05 )
1.96

>>> stats.inverse_student( 0.05, 6 )
1.943

>>> stats.inverse_fisher( 0.05, 6, 6 )
4.284

>>> x = [ 0.388, 0.389, 0.410 ]
>>> na = len( x )
>>> ma, sa = stats.stats( x )
>>> nb = na - 1
>>> mb, sb = stats.stats( x[0:-1] )
>>> stats.i_test( 1, x[-1], .0, nb, mb, sb )
(0.410000,0.410000,0.410000) overlaps (0.385343,0.388500,0.391657): False
False

>>> stats.t_test( 2, x[-1], .0, nb, mb, sb )
t_calc(43.000000) <= t_tab(1,0.0500 = 6.314000): False
False

>>> stats.f_test( na, sa, nb, sb )
f_calc(308.666667) <= f_tab(2,1,0.0500 = 199.499000): False
False

>>> o = stats.k_means( [ 7, 4, 10, 16, 13, 7, 3, 5, 7, 3, 13, 14, 12, 11, 10, 7, 7, 5, 3, 3 ] ) 
>>> o.find_centers( 2 )
{0: [10, 16, 13, 13, 14, 12, 11, 10], 1: [7, 4, 7, 3, 5, 7, 3, 7, 7, 5, 3, 3]}
>>> o.find_centers( 3 )
{0: [16, 13, 13, 14, 12], 1: [7, 4, 7, 3, 5, 7, 3, 7, 7, 5, 3, 3], 2: [10, 11, 10]}
>>> o.find_centers( 4 )
{0: [16, 14], 1: [7, 7, 5, 7, 7, 7, 5], 2: [10, 13, 13, 12, 11, 10], 3: [4, 3, 3, 3, 3]}

>>> x = [ [ 7, 4, 10, 16, 13 ], [ 7, 3, 5, 7, 3 ], [ 13, 14, 12, 11, 10 ], [ 7, 7, 5, 3, 3 ] ]
>>> matrix.mprint( matrix.T( sum( x, [] ), 4, 5 ), 5, 4 )
5 rows x 4 columns
   7.000    7.000   13.000    7.000 
   4.000    3.000   14.000    7.000 
  10.000    5.000   12.000    5.000 
  16.000    7.000   11.000    3.000 
  13.000    3.000   10.000    3.000 

>>> o = stats.PCA( x )
[Cov]
4 rows x 4 columns
  18.000    2.400   -5.400   -7.200 
   2.400    3.200    0.000    0.000 
  -5.400    0.000    2.000    2.400 
  -7.200    0.000    2.400    3.200 
Eig-val:  [22.849549071130916, 3.3744840968681555, 0.17596683200093394, 1.8559979853574662e-16]
Eig-vec:
4 rows x 4 columns
   0.887    0.068    0.240    0.389 
   0.108    0.931   -0.191   -0.291 
  -0.271    0.223    0.936    0.000 
  -0.358    0.280   -0.171    0.874 

>>> o.select( [ 0, 1, 2 ] )
[Q]
4 rows x 3 columns
   0.887    0.068    0.240 
   0.108    0.931   -0.191 
  -0.271    0.223    0.936 
  -0.358    0.280   -0.171 
[reduced]
5 rows x 3 columns
   6.569    7.443   11.492 
   3.204    3.739   12.470 
  10.000    5.000   12.000 
  16.525    6.484   12.466 
  13.702    2.333   11.572 
\end{pyglist}
